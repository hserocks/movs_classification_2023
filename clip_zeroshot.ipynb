{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install git+https://github.com/openai/CLIP.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Bear', 'Brown bear', 'Bull', 'Camel', 'Canary', 'Cat', 'Caterpillar', 'Cattle', 'Centipede', 'Cheetah', 'Chicken', 'Crab', 'Crocodile', 'Deer', 'Dog', 'Duck', 'Eagle', 'Elephant', 'Fish', 'Fox', 'Frog', 'Giraffe', 'Goat', 'Goldfish', 'Goose', 'Hamster', 'Harbor seal', 'Hedgehog', 'Hippopotamus', 'Horse', 'Jaguar', 'Jellyfish', 'Kangaroo', 'Koala', 'Ladybug', 'Leopard', 'Lion', 'Lizard', 'Lynx', 'Magpie', 'Monkey', 'Moths and butterflies', 'Mouse', 'Mule', 'Ostrich', 'Otter', 'Owl', 'Panda', 'Parrot', 'Penguin', 'Pig', 'Polar bear', 'Rabbit', 'Raccoon', 'Raven', 'Red panda', 'Rhinoceros', 'Scorpion', 'Sea lion', 'Sea turtle', 'Seahorse', 'Shark', 'Sheep', 'Shrimp', 'Snail', 'Snake', 'Sparrow', 'Spider', 'Squid', 'Squirrel', 'Starfish', 'Swan', 'Tick', 'Tiger', 'Tortoise', 'Turkey', 'Turtle', 'Whale', 'Woodpecker', 'Worm', 'Zebra']\n"
     ]
    }
   ],
   "source": [
    "# read categories\n",
    "import os\n",
    "#os.chdir('/movs_classification_2023/')\n",
    "\n",
    "import json\n",
    "\n",
    "with open('class_to_idx.json', 'r') as f:\n",
    "    class_to_idx = json.load(f)\n",
    "\n",
    "cat_names = list(class_to_idx.keys())\n",
    "print(cat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import clip\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)\n",
    "\n",
    "\n",
    "categories = cat_names\n",
    "\n",
    "# Preprocess the category names into a format suitable for CLIP\n",
    "category_texts = clip.tokenize(categories).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image is classified as: Camel\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess  image\n",
    "image_path = \"sample/4.jpg\"\n",
    "image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "# Encode the image and the category texts\n",
    "with torch.no_grad():\n",
    "    image_features = model.encode_image(image)\n",
    "    text_features = model.encode_text(category_texts)\n",
    "\n",
    "# Compute the similarity between the image and each category\n",
    "image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "# Find the category with the highest similarity score\n",
    "best_category_index = similarities.argmax(dim=-1).item()\n",
    "best_category = categories[best_category_index]\n",
    "\n",
    "print(f\"The image is classified as: {best_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# top 5 categories CLIP\n",
    "\n",
    "def get_categories_clip(new_image_path):\n",
    "    import pandas as pd\n",
    "    from PIL import Image\n",
    "    import clip\n",
    "    import json\n",
    "\n",
    "    import torch\n",
    "    from torchvision import transforms\n",
    "    import torchvision.models as models\n",
    "    import torch.nn as nn\n",
    "    import torch.nn.functional as F\n",
    "    \n",
    "    # Check if GPU is available\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    \n",
    "    # Load CLIP model\n",
    "    model, preprocess = clip.load(\"ViT-B/32\", device)\n",
    "    \n",
    "    # Load cat names\n",
    "    with open('class_to_idx.json', 'r') as f:\n",
    "        class_to_idx = json.load(f)\n",
    "\n",
    "    # Get and preprocess cat names into a format suitable for CLIP\n",
    "    cat_names = list(class_to_idx.keys())\n",
    "    categories = cat_names    \n",
    "    category_texts = clip.tokenize(categories).to(device)\n",
    "\n",
    "    # Load image\n",
    "    image_path = new_image_path  # test image path\n",
    "    image = preprocess(Image.open(image_path)).unsqueeze(0).to(device)\n",
    "\n",
    "    # Encode the image and the category texts\n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(image)\n",
    "        text_features = model.encode_text(category_texts)\n",
    "\n",
    "    # Compute the similarity between the image and each category\n",
    "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
    "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
    "    similarities = (100.0 * image_features @ text_features.T).softmax(dim=-1)\n",
    "\n",
    "    # Get the top 5 categories and their probabilities\n",
    "    top_num = 5\n",
    "    top_prob, top_catid = torch.topk(similarities, top_num)\n",
    "\n",
    "    # Convert to Python data types\n",
    "    top_prob = top_prob.cpu().numpy()[0]\n",
    "    top_catid = top_catid.cpu().numpy()[0]\n",
    "\n",
    "    # Map indices to class names and prepare predictions\n",
    "    predictions = []\n",
    "    for i in range(top_num):\n",
    "        predicted_class_name = categories[top_catid[i]]\n",
    "        predicted_probability = top_prob[i]\n",
    "        predictions.append({'Category ID': predicted_class_name,\n",
    "                            'Probability': predicted_probability})\n",
    "\n",
    "    df = pd.DataFrame(predictions)\n",
    "    df_string = df.to_string(index=False)\n",
    "    print(df_string)\n",
    "\n",
    "    return df_string  # return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Category ID  Probability\n",
      "    Chicken     0.958496\n",
      "     Turkey     0.011513\n",
      "       Duck     0.005699\n",
      "     Canary     0.004105\n",
      "     Parrot     0.003298\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Category ID  Probability\\n    Chicken     0.958496\\n     Turkey     0.011513\\n       Duck     0.005699\\n     Canary     0.004105\\n     Parrot     0.003298'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_categories_clip('sample/5.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected model: CLIP (zero-shot). Starting testing\n",
      "Using 32 workers for data loading\n",
      "Using cuda\n",
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing: 100%|██████████| 346/346 [00:10<00:00, 32.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.806483745359051\n",
      "Precision: 0.8725240529868838\n",
      "Recall: 0.806483745359051\n",
      "F1 Score: 0.8252605390772069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'accuracy': 0.806483745359051,\n",
       " 'precision': 0.8725240529868838,\n",
       " 'recall': 0.806483745359051,\n",
       " 'f1': 0.8252605390772069}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing CLIP model\n",
    "from utils.clip import test_clip\n",
    "from utils.vit import prepare_data_vit\n",
    "\n",
    "\n",
    "image_folder_path='Data_small'\n",
    "\n",
    "print('Selected model: CLIP (zero-shot). Starting testing')\n",
    "device, no_of_classes, train_loader, test_loader, dataloader = \\\n",
    "    prepare_data_vit(image_folder_path) # CLIP uses the same approach as VIT (so it's okay)\n",
    "# Load cat names\n",
    "class_to_idx_path = 'class_to_idx.json'\n",
    "\n",
    "metrics_dict = test_clip(class_to_idx_path, test_loader)\n",
    "metrics_dict\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
